{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9ECedSK0fdfcNLOOEBklY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##BASE##\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from sklearn.metrics import confusion_matrix, classification_report\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","classified_count = 0\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    return ' '.join(words)\n","\n","def extract_text_by_subject_id(folder_path):\n","    global classified_count\n","    data = []\n","    subject_dict = {}\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.xml'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                tree = ET.parse(file_path)\n","                root = tree.getroot()\n","\n","                if root.tag == 'INDIVIDUAL':\n","                    subject_elem = root.find('./ID')\n","                    subject_id = subject_elem.text.strip() if subject_elem is not None and subject_elem.text else \"Missing Subject ID\"\n","\n","                    for writing in root.findall('.//WRITING'):\n","                        text_elem = writing.find('TEXT')\n","\n","                        if text_elem is not None and text_elem.text:\n","                            text = preprocess_text(text_elem.text.strip())\n","\n","                            if subject_id not in subject_dict:\n","                                subject_dict[subject_id] = []\n","                            subject_dict[subject_id].append(text)\n","\n","\n","            except ET.ParseError as e:\n","                print(f\"Error parsing {filename}: {e}\")\n","            except Exception as e:\n","                print(f\"An error occurred with {filename}: {e}\")\n","\n","    for subject_id, texts in subject_dict.items():\n","        term_freq = Counter()\n","        combined_texts = ' | '.join(texts)\n","        combined_texts = preprocess_text(combined_texts)\n","        term_freq.update(combined_texts.split())\n","        text_by_freq = term_freq.most_common()\n","\n","        term_freq_str = \", \".join([f\"{term}:{freq}\" for term, freq in text_by_freq])\n","        data.append({'ID': subject_id, 'TEXT': combined_texts, 'Term-Frequency ': term_freq_str})\n","\n","    df = pd.DataFrame(data)\n","    return pd.DataFrame(data)\n","def plot_sentiment_distribution(df):\n","    sns.countplot(x='Predicted Sentiment', data=df)\n","    plt.title(\"Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Count\")\n","    plt.show()\n","def plot_wordcloud(df, sentiment_value, sentiment_label):\n","    sentiment_texts = ' '.join(df[df['Predicted Sentiment'] == sentiment_value]['TEXT'].dropna())\n","    if not sentiment_texts.strip():\n","        print(f\"No text entries found for {sentiment_label} sentiment.\")\n","        return\n","\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.title(f\"Word Cloud for {sentiment_label.capitalize()} Sentiments\")\n","    plt.show()\n","\n","def load_ground_truth(ground_truth_file):\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            if len(parts) == 2:\n","                subject_id, label = parts\n","                ground_truth[subject_id] = int(label)\n","    return ground_truth\n","\n","def evaluate_model(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    conf = confusion_matrix(y_true, y_pred)\n","\n","    print(f\"Confusion Matrix: {conf.tolist()}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","def derive_pos_tags(df):\n","    pos_data = []\n","    for index, row in df.iterrows():\n","        words = row['TEXT'].split()\n","        pos_tags = nltk.pos_tag(words)\n","        pos_data.extend([(word, tag) for word, tag in pos_tags])\n","    pos_df = pd.DataFrame(pos_data, columns=['Word', 'POS Tag'])\n","    pos_df.to_csv('pathtofile', index=False)\n","    print(\"POS tags saved to pos_tags_base.csv\")\n","\n","def save_predictions_to_csv(test_df, predictions, output_file):\n","    test_df['Predicted Sentiment'] = predictions\n","    test_df[['ID', 'TEXT', 'Predicted Sentiment', 'Term-Frequency ']].to_csv(output_file, index=False)\n","    print(f\"Predictions saved to {output_file}\")\n","\n","folder_path = 'pathtofolder'\n","ground_truth_file = 'pathtofolder'\n","\n","\n","df = extract_text_by_subject_id(folder_path)\n","\n","ground_truth = load_ground_truth(ground_truth_file)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_texts = train_df['TEXT'].tolist()\n","train_labels = [ground_truth[id_] for id_ in train_df['ID']]\n","test_texts = test_df['TEXT'].tolist()\n","test_labels = [ground_truth[id_] for id_ in test_df['ID']]\n","\n","\n","# Classifier setup\n","vectorizer = TfidfVectorizer(max_features=1000)\n","svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n","\n","train_vectors = vectorizer.fit_transform(train_texts)\n","test_vectors = vectorizer.transform(test_texts)\n","\n","svm_model.fit(train_vectors, train_labels)\n","\n","test_predictions = svm_model.predict(test_vectors)\n","\n","print(\"\\nEvaluation on Testing Set:\")\n","evaluate_model(test_labels, test_predictions)\n","\n","# Save predictions\n","output_file = 'pathtofile'\n","save_predictions_to_csv(test_df, test_predictions, output_file)\n","plot_wordcloud(test_df, sentiment_value=1, sentiment_label='positive')\n","plot_wordcloud(test_df, sentiment_value=0, sentiment_label='negative')\n","plot_sentiment_distribution(df)\n","\n","print(classified_count)"],"metadata":{"id":"-5L2c9jOe2gL","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"error","timestamp":1741426454108,"user_tz":-60,"elapsed":5865,"user":{"displayName":"Derin Akcan","userId":"18149776131054092282"}},"outputId":"e327b0cf-dfb6-41a6-cdb0-dba6495a2265"},"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-754aeb20c333>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m###########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m \"\"\"\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_entity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxent_NE_Chunker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpChunkParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/chunk/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m##//////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParserI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/chunk/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstr2tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggerI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstr2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple2str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m from nltk.tag.sequential import (\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mSequentialBackoffTagger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mContextTagger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjsontags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeaturesetTaggerI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggerI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/classify/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositivenaivebayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPositiveNaiveBayesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrte_classify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRTEFeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrte_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrte_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikitlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSenna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextcat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextCat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/classify/scikitlearn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_sqr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_verbose_message\u001b[0;34m(message, verbosity, *args)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#HAIR AND SKIN##\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.pipeline import make_pipeline\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","keywords = [\n","    \"skin\", \"ugly\", \"hair\", \"hate my face\", \"pimples\", \"dark circles\", \"hyperpigmentation\",\n","    \"acne scars\", \"pick my skin\", \"wrinkles \", \"aging\", \"my confidence\", \"balding\", \"hair loss\", \"hair pulling\", \"acne\",\n","    \"hair transplants \",\"going bald\" , \"perfect hair\", \"dandruff\", \"dark spots\"\n","    \"picking skin\", \"gone bald\"\n","]\n","classified_count = 0\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    return ' '.join(words)\n","\n","def extract_text_by_subject_id(folder_path):\n","    global classified_count\n","    data = []\n","    subject_dict = {}\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.xml'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                tree = ET.parse(file_path)\n","                root = tree.getroot()\n","\n","                if root.tag == 'INDIVIDUAL':\n","                    subject_elem = root.find('./ID')\n","                    subject_id = subject_elem.text.strip() if subject_elem is not None and subject_elem.text else \"Missing Subject ID\"\n","\n","                    for writing in root.findall('.//WRITING'):\n","                        text_elem = writing.find('TEXT')\n","\n","                        if text_elem is not None and text_elem.text:\n","                            text = text_elem.text.strip()\n","\n","                            if any(keyword in text.lower() for keyword in keywords):\n","                                if subject_id not in subject_dict:\n","                                    subject_dict[subject_id] = []\n","                                subject_dict[subject_id].append(text)\n","                                classified_count += 1\n","\n","            except ET.ParseError as e:\n","                print(f\"Error parsing {filename}: {e}\")\n","            except Exception as e:\n","                print(f\"An error occurred with {filename}: {e}\")\n","\n","    for subject_id, texts in subject_dict.items():\n","        term_freq = Counter()\n","        combined_texts = ' | '.join(texts)\n","        combined_texts = preprocess_text(combined_texts)\n","        term_freq.update(combined_texts.split())\n","        text_by_freq = term_freq.most_common()\n","        term_freq_str = \", \".join([f\"{term}:{freq}\" for term, freq in text_by_freq])\n","        data.append({'ID': subject_id, 'TEXT': combined_texts, 'Term-Frequency ': term_freq_str})\n","\n","    df = pd.DataFrame(data)\n","    return df\n","\n","def plot_sentiment_distribution(df):\n","    sns.countplot(x='Predicted Sentiment', data=df)\n","    plt.title(\"Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Count\")\n","    plt.show()\n","def plot_wordcloud(df, sentiment_value, sentiment_label):\n","    sentiment_texts = ' '.join(df[df['Predicted Sentiment'] == sentiment_value]['TEXT'].dropna())\n","    if not sentiment_texts.strip():\n","        print(f\"No text entries found for {sentiment_label} sentiment.\")\n","        return\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.title(f\"Word Cloud for {sentiment_label.capitalize()} Sentiments\")\n","    plt.show()\n","\n","def load_ground_truth(ground_truth_file):\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            if len(parts) == 2:\n","                subject_id, label = parts\n","                ground_truth[subject_id] = int(label)\n","    return ground_truth\n","\n","\n","def evaluate_model(y_true, y_pred):\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_true, y_pred))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_true, y_pred))\n","\n","def derive_pos_tags(df):\n","    pos_data = []\n","    for index, row in df.iterrows():\n","        words = row['TEXT'].split()\n","        pos_tags = nltk.pos_tag(words)\n","        pos_data.extend([(word, tag) for word, tag in pos_tags])\n","    pos_df = pd.DataFrame(pos_data, columns=['Word', 'POS Tag'])\n","    pos_df.to_csv('pathtofile', index=False)\n","    print(\"POS tags saved to pos_tags_hairandskin.csv\")\n","\n","def save_predictions_to_csv(test_df, predictions, output_file):\n","    test_df['Predicted Sentiment'] = predictions\n","    test_df[['ID', 'TEXT', 'Predicted Sentiment', 'Term-Frequency ']].to_csv(output_file, index=False)\n","    print(f\"Predictions saved to {output_file}\")\n","\n","folder_path = 'pathtofile'\n","ground_truth_file = 'pathtofile'\n","\n","df = extract_text_by_subject_id(folder_path)\n","\n","ground_truth = load_ground_truth(ground_truth_file)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_texts = train_df['TEXT'].tolist()\n","train_labels = [ground_truth[id_] for id_ in train_df['ID']]\n","test_texts = test_df['TEXT'].tolist()\n","test_labels = [ground_truth[id_] for id_ in test_df['ID']]\n","\n","\n","def save_all_sentiments_to_csv(df, model, vectorizer, output_file):\n","    text_vectors = vectorizer.transform(df['TEXT'])\n","    predictions = model.predict(text_vectors)\n","    results_df = df.copy()\n","    results_df['Predicted Sentiment'] = predictions\n","    results_df[['ID', 'TEXT', 'Predicted Sentiment']].to_csv(output_file, index=False)\n","    print(f\"Sentiment predictions saved to {output_file}\")\n","\n","# Classifier setup\n","vectorizer = TfidfVectorizer(max_features=1000)\n","svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n","\n","train_vectors = vectorizer.fit_transform(train_texts)\n","test_vectors = vectorizer.transform(test_texts)\n","\n","svm_model.fit(train_vectors, train_labels)\n","\n","test_predictions = svm_model.predict(test_vectors)\n","\n","print(\"\\nEvaluation on Testing Set:\")\n","evaluate_model(test_labels, test_predictions)\n","\n","# Save predictions\n","output_file = 'pathtofile'\n","save_predictions_to_csv(test_df, test_predictions, output_file)\n","\n","df['Predicted Sentiment'] = svm_model.predict(vectorizer.transform(df['TEXT']))\n","\n","plot_wordcloud(test_df, sentiment_value=1, sentiment_label='positive')\n","plot_wordcloud(test_df, sentiment_value=0, sentiment_label='negative')\n","plot_sentiment_distribution(df)\n","\n","print(f\"Classified count: {classified_count}\")"],"metadata":{"id":"Pw6JKuvk_3ma","executionInfo":{"status":"aborted","timestamp":1741426454112,"user_tz":-60,"elapsed":6060,"user":{"displayName":"Derin Akcan","userId":"18149776131054092282"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##WEIGHT##\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from sklearn.metrics import confusion_matrix, classification_report\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","keywords = [\n","    \"weight loss\", \"weight\", \"look ugly\",\n","    \"abnormal body\", \"overweight\", \"body dysmorphic disorder\",\n","    \"hate my body\", \"hated my body\", \"underweight\",\n","    \"too skinny\", \"too fat\", \"weight gain\"\n","]\n","classified_count = 0\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    return ' '.join(words)\n","classified_count = []\n","def extract_text_by_subject_id(folder_path):\n","    global classified_count\n","    data = []\n","    subject_dict = {}\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.xml'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                tree = ET.parse(file_path)\n","                root = tree.getroot()\n","\n","                if root.tag == 'INDIVIDUAL':\n","                    subject_elem = root.find('./ID')\n","                    subject_id = subject_elem.text.strip() if subject_elem is not None and subject_elem.text else \"Missing Subject ID\"\n","\n","                    for writing in root.findall('.//WRITING'):\n","                        text_elem = writing.find('TEXT')\n","\n","                        if text_elem is not None and text_elem.text:\n","                            text = text_elem.text.strip()\n","\n","                            if any(keyword in text.lower() for keyword in keywords):\n","                                if subject_id not in subject_dict:\n","                                    subject_dict[subject_id] = []\n","                                subject_dict[subject_id].append(text)\n","                                classified_count += 1\n","\n","            except ET.ParseError as e:\n","                print(f\"Error parsig {filename}: {e}\")\n","            except Exception as e:\n","                print(f\"An error occurred with {filename}: {e}\")\n","\n","    for subject_id, texts in subject_dict.items():\n","        term_freq = Counter()\n","        combined_texts = ' | '.join(texts)\n","        combined_texts = preprocess_text(combined_texts)\n","        term_freq.update(combined_texts.split())\n","        text_by_freq = term_freq.most_common()\n","        term_freq_str = \", \".join([f\"{term}:{freq}\" for term, freq in text_by_freq])\n","        data.append({'ID': subject_id, 'TEXT': combined_texts, 'Term-Frequency': term_freq_str})\n","\n","    df = pd.DataFrame(data)\n","    return df\n","\n","def plot_sentiment_distribution(df):\n","    sns.countplot(x='Predicted Sentiment', data=df)\n","    plt.title(\"Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Count\")\n","    plt.show()\n","def plot_wordcloud(df, sentiment_value, sentiment_label):\n","    sentiment_texts = ' '.join(df[df['Predicted Sentiment'] == sentiment_value]['TEXT'].dropna())\n","\n","    if not sentiment_texts.strip():\n","        print(f\"No text entries found for {sentiment_label} sentiment.\")\n","        return\n","\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.title(f\"Word Cloud for {sentiment_label.capitalize()} Sentiments\")\n","    plt.show()\n","\n","def load_ground_truth(ground_truth_file):\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            if len(parts) == 2:\n","                subject_id, label = parts\n","                ground_truth[subject_id] = int(label)\n","    return ground_truth\n","\n","\n","def evaluate_model(y_true, y_pred):\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_true, y_pred))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_true, y_pred))\n","\n","def derive_pos_tags(df):\n","    pos_data = []\n","    for index, row in df.iterrows():\n","        words = row['TEXT'].split()\n","        pos_tags = nltk.pos_tag(words)\n","        pos_data.extend([(word, tag) for word, tag in pos_tags])\n","    pos_df = pd.DataFrame(pos_data, columns=['Word', 'POS Tag'])\n","    pos_df.to_csv('pathtofile', index=False)\n","    print(\"POS tags saved to pos_tags_weight.csv\")\n","\n","def save_predictions_to_csv(test_df, predictions, output_file):\n","    test_df['Predicted Sentiment'] = predictions\n","    test_df[['ID', 'TEXT', 'Predicted Sentiment', 'Term-Frequency']].to_csv(output_file, index=False)\n","    print(f\"Predictions saved to {output_file}\")\n","\n","folder_path = 'pathtofile'\n","ground_truth_file = 'pathtofile'\n","\n","all_xml_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.xml')]\n","\n","#skipped_files = set(all_xml_files) - set(processed_files)\n","\n","df = extract_text_by_subject_id(folder_path)\n","\n","ground_truth = load_ground_truth(ground_truth_file)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_texts = train_df['TEXT'].tolist()\n","train_labels = [ground_truth[id_] for id_ in train_df['ID']]\n","test_texts = test_df['TEXT'].tolist()\n","test_labels = [ground_truth[id_] for id_ in test_df['ID']]\n","\n","def save_all_sentiments_to_csv(df, model, vectorizer, output_file):\n","    text_vectors = vectorizer.transform(df['TEXT'])\n","    predictions = model.predict(text_vectors)\n","    results_df = df.copy()\n","    results_df['Predicted Sentiment'] = predictions\n","    results_df[['ID', 'TEXT', 'Predicted Sentiment']].to_csv(output_file, index=False)\n","    print(f\"Sentiment predictions saved to {output_file}\")\n","\n","# Classifier setup\n","vectorizer = TfidfVectorizer(max_features=1000)\n","svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n","\n","train_vectors = vectorizer.fit_transform(train_texts)\n","test_vectors = vectorizer.transform(test_texts)\n","\n","svm_model.fit(train_vectors, train_labels)\n","\n","test_predictions = svm_model.predict(test_vectors)\n","\n","print(\"\\nEvaluation on Testing Set:\")\n","evaluate_model(test_labels, test_predictions)\n","\n","# Save predictions\n","output_file = 'pathtofile'\n","save_predictions_to_csv(test_df, test_predictions, output_file)\n","\n","df['Predicted Sentiment'] = svm_model.predict(vectorizer.transform(df['TEXT']))\n","\n","plot_wordcloud(test_df, sentiment_value=1, sentiment_label='positive')\n","plot_wordcloud(test_df, sentiment_value=0, sentiment_label='negative')\n","plot_sentiment_distribution(df)\n","\n","print(f\"Classified count: {classified_count}\")\n"],"metadata":{"id":"Uz1MGkK2E1HP"},"execution_count":null,"outputs":[]}]}