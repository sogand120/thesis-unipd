{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrVXzFBZAsiexVkwoXvsAD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##BASE##\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from sklearn.metrics import confusion_matrix, classification_report\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","classified_count = 0\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    return ' '.join(words)\n","\n","def extract_text_by_subject_id(folder_path):\n","    global classified_count\n","    data = []\n","    subject_dict = {}\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.xml'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                tree = ET.parse(file_path)\n","                root = tree.getroot()\n","\n","                if root.tag == 'INDIVIDUAL':\n","                    subject_elem = root.find('./ID')\n","                    subject_id = subject_elem.text.strip() if subject_elem is not None and subject_elem.text else \"Missing Subject ID\"\n","\n","                    for writing in root.findall('.//WRITING'):\n","                        text_elem = writing.find('TEXT')\n","\n","                        if text_elem is not None and text_elem.text:\n","                            text = preprocess_text(text_elem.text.strip())\n","\n","                            if subject_id not in subject_dict:\n","                                subject_dict[subject_id] = []\n","                            subject_dict[subject_id].append(text)\n","\n","\n","            except ET.ParseError as e:\n","                print(f\"Error parsing {filename}: {e}\")\n","            except Exception as e:\n","                print(f\"An error occurred with {filename}: {e}\")\n","\n","    for subject_id, texts in subject_dict.items():\n","        term_freq = Counter()\n","        combined_texts = ' | '.join(texts)\n","        combined_texts = preprocess_text(combined_texts)\n","        term_freq.update(combined_texts.split())\n","        text_by_freq = term_freq.most_common()\n","\n","        term_freq_str = \", \".join([f\"{term}:{freq}\" for term, freq in text_by_freq])\n","        data.append({'ID': subject_id, 'TEXT': combined_texts, 'Term-Frequency ': term_freq_str})\n","\n","    df = pd.DataFrame(data)\n","    return pd.DataFrame(data)\n","def plot_sentiment_distribution(df):\n","    sns.countplot(x='Predicted Sentiment', data=df)\n","    plt.title(\"Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Count\")\n","    plt.show()\n","def plot_wordcloud(df, sentiment_value, sentiment_label):\n","    sentiment_texts = ' '.join(df[df['Predicted Sentiment'] == sentiment_value]['TEXT'].dropna())\n","    if not sentiment_texts.strip():\n","        print(f\"No text entries found for {sentiment_label} sentiment.\")\n","        return\n","\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.title(f\"Word Cloud for {sentiment_label.capitalize()} Sentiments\")\n","    plt.show()\n","\n","def load_ground_truth(ground_truth_file):\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            if len(parts) == 2:\n","                subject_id, label = parts\n","                ground_truth[subject_id] = int(label)\n","    return ground_truth\n","\n","def evaluate_model(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    conf = confusion_matrix(y_true, y_pred)\n","\n","    print(f\"Confusion Matrix: {conf.tolist()}\")\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","def derive_pos_tags(df):\n","    pos_data = []\n","    for index, row in df.iterrows():\n","        words = row['TEXT'].split()\n","        pos_tags = nltk.pos_tag(words)\n","        pos_data.extend([(word, tag) for word, tag in pos_tags])\n","    pos_df = pd.DataFrame(pos_data, columns=['Word', 'POS Tag'])\n","    pos_df.to_csv('pathtofile', index=False)\n","    print(\"POS tags saved to pos_tags_base.csv\")\n","\n","def save_predictions_to_csv(test_df, predictions, output_file):\n","    test_df['Predicted Sentiment'] = predictions\n","    test_df[['ID', 'TEXT', 'Predicted Sentiment', 'Term-Frequency ']].to_csv(output_file, index=False)\n","    print(f\"Predictions saved to {output_file}\")\n","\n","folder_path = 'pathtofolder'\n","ground_truth_file = 'pathtofolder'\n","\n","\n","df = extract_text_by_subject_id(folder_path)\n","\n","ground_truth = load_ground_truth(ground_truth_file)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_texts = train_df['TEXT'].tolist()\n","train_labels = [ground_truth[id_] for id_ in train_df['ID']]\n","test_texts = test_df['TEXT'].tolist()\n","test_labels = [ground_truth[id_] for id_ in test_df['ID']]\n","\n","\n","# Classifier setup\n","vectorizer = TfidfVectorizer(max_features=1000)\n","svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n","\n","train_vectors = vectorizer.fit_transform(train_texts)\n","test_vectors = vectorizer.transform(test_texts)\n","\n","svm_model.fit(train_vectors, train_labels)\n","\n","test_predictions = svm_model.predict(test_vectors)\n","\n","print(\"\\nEvaluation on Testing Set:\")\n","evaluate_model(test_labels, test_predictions)\n","\n","# Save predictions\n","output_file = 'pathtofile'\n","save_predictions_to_csv(test_df, test_predictions, output_file)\n","plot_wordcloud(test_df, sentiment_value=1, sentiment_label='positive')\n","plot_wordcloud(test_df, sentiment_value=0, sentiment_label='negative')\n","plot_sentiment_distribution(df)\n","\n","print(classified_count)"],"metadata":{"id":"j_zAn2XP3oWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#HAIR AND SKIN##\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.pipeline import make_pipeline\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","keywords = [\n","    \"skin\", \"ugly\", \"hair\", \"hate my face\", \"pimples\", \"dark circles\",\n","    \"hyperpigmentation\",\"acne scars\", \"pick my skin\", \"wrinkles \", \"aging\",\n","    \"my confidence\", \"balding\", \"hair loss\", \"hair pulling\", \"acne\",\n","    \"hair transplants \",\"going bald\" , \"perfect hair\", \"dandruff\", \"dark spots\"\n","    \"picking skin\", \"gone bald\"\n","]\n","classified_count = 0\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    return ' '.join(words)\n","\n","def extract_text_by_subject_id(folder_path):\n","    global classified_count\n","    data = []\n","    subject_dict = {}\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.xml'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                tree = ET.parse(file_path)\n","                root = tree.getroot()\n","\n","                if root.tag == 'INDIVIDUAL':\n","                    subject_elem = root.find('./ID')\n","                    subject_id = subject_elem.text.strip() if subject_elem is not None and subject_elem.text else \"Missing Subject ID\"\n","\n","                    for writing in root.findall('.//WRITING'):\n","                        text_elem = writing.find('TEXT')\n","\n","                        if text_elem is not None and text_elem.text:\n","                            text = text_elem.text.strip()\n","\n","                            if any(keyword in text.lower() for keyword in keywords):\n","                                if subject_id not in subject_dict:\n","                                    subject_dict[subject_id] = []\n","                                subject_dict[subject_id].append(text)\n","                                classified_count += 1\n","\n","            except ET.ParseError as e:\n","                print(f\"Error parsing {filename}: {e}\")\n","            except Exception as e:\n","                print(f\"An error occurred with {filename}: {e}\")\n","\n","    for subject_id, texts in subject_dict.items():\n","        term_freq = Counter()\n","        combined_texts = ' | '.join(texts)\n","        combined_texts = preprocess_text(combined_texts)\n","        term_freq.update(combined_texts.split())\n","        text_by_freq = term_freq.most_common()\n","        term_freq_str = \", \".join([f\"{term}:{freq}\" for term, freq in text_by_freq])\n","        data.append({'ID': subject_id, 'TEXT': combined_texts, 'Term-Frequency ': term_freq_str})\n","\n","    df = pd.DataFrame(data)\n","    return df\n","\n","def plot_sentiment_distribution(df):\n","    sns.countplot(x='Predicted Sentiment', data=df)\n","    plt.title(\"Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Count\")\n","    plt.show()\n","def plot_wordcloud(df, sentiment_value, sentiment_label):\n","    sentiment_texts = ' '.join(df[df['Predicted Sentiment'] == sentiment_value]['TEXT'].dropna())\n","    if not sentiment_texts.strip():\n","        print(f\"No text entries found for {sentiment_label} sentiment.\")\n","        return\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.title(f\"Word Cloud for {sentiment_label.capitalize()} Sentiments\")\n","    plt.show()\n","\n","def load_ground_truth(ground_truth_file):\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            if len(parts) == 2:\n","                subject_id, label = parts\n","                ground_truth[subject_id] = int(label)\n","    return ground_truth\n","\n","\n","def evaluate_model(y_true, y_pred):\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_true, y_pred))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_true, y_pred))\n","\n","def derive_pos_tags(df):\n","    pos_data = []\n","    for index, row in df.iterrows():\n","        words = row['TEXT'].split()\n","        pos_tags = nltk.pos_tag(words)\n","        pos_data.extend([(word, tag) for word, tag in pos_tags])\n","    pos_df = pd.DataFrame(pos_data, columns=['Word', 'POS Tag'])\n","    pos_df.to_csv('pathtofile', index=False)\n","    print(\"POS tags saved to pos_tags_hairandskin.csv\")\n","\n","def save_predictions_to_csv(test_df, predictions, output_file):\n","    test_df['Predicted Sentiment'] = predictions\n","    test_df[['ID', 'TEXT', 'Predicted Sentiment', 'Term-Frequency ']].to_csv(output_file, index=False)\n","    print(f\"Predictions saved to {output_file}\")\n","\n","folder_path = 'pathtofile'\n","ground_truth_file = 'pathtofile'\n","\n","df = extract_text_by_subject_id(folder_path)\n","\n","ground_truth = load_ground_truth(ground_truth_file)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_texts = train_df['TEXT'].tolist()\n","train_labels = [ground_truth[id_] for id_ in train_df['ID']]\n","test_texts = test_df['TEXT'].tolist()\n","test_labels = [ground_truth[id_] for id_ in test_df['ID']]\n","\n","\n","def save_all_sentiments_to_csv(df, model, vectorizer, output_file):\n","    text_vectors = vectorizer.transform(df['TEXT'])\n","    predictions = model.predict(text_vectors)\n","    results_df = df.copy()\n","    results_df['Predicted Sentiment'] = predictions\n","    results_df[['ID', 'TEXT', 'Predicted Sentiment']].to_csv(output_file, index=False)\n","    print(f\"Sentiment predictions saved to {output_file}\")\n","\n","# Classifier setup\n","vectorizer = TfidfVectorizer(max_features=1000)\n","svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n","\n","train_vectors = vectorizer.fit_transform(train_texts)\n","test_vectors = vectorizer.transform(test_texts)\n","\n","svm_model.fit(train_vectors, train_labels)\n","\n","test_predictions = svm_model.predict(test_vectors)\n","\n","print(\"\\nEvaluation on Testing Set:\")\n","evaluate_model(test_labels, test_predictions)\n","\n","# Save predictions\n","output_file = 'pathtofile'\n","save_predictions_to_csv(test_df, test_predictions, output_file)\n","\n","df['Predicted Sentiment'] = svm_model.predict(vectorizer.transform(df['TEXT']))\n","\n","plot_wordcloud(test_df, sentiment_value=1, sentiment_label='positive')\n","plot_wordcloud(test_df, sentiment_value=0, sentiment_label='negative')\n","plot_sentiment_distribution(df)\n","\n","print(f\"Classified count: {classified_count}\")"],"metadata":{"id":"Pw6JKuvk_3ma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##WEIGHT##\n","import os\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from collections import Counter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","from sklearn.metrics import confusion_matrix, classification_report\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","keywords = [\n","    \"weight loss\", \"weight\", \"look ugly\",\n","    \"abnormal body\", \"overweight\", \"body dysmorphic disorder\",\n","    \"hate my body\", \"hated my body\", \"underweight\",\n","    \"too skinny\", \"too fat\", \"weight gain\"\n","]\n","classified_count = 0\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    return ' '.join(words)\n","classified_count = []\n","def extract_text_by_subject_id(folder_path):\n","    global classified_count\n","    data = []\n","    subject_dict = {}\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith('.xml'):\n","            file_path = os.path.join(folder_path, filename)\n","            try:\n","                tree = ET.parse(file_path)\n","                root = tree.getroot()\n","\n","                if root.tag == 'INDIVIDUAL':\n","                    subject_elem = root.find('./ID')\n","                    subject_id = subject_elem.text.strip() if subject_elem is not None and subject_elem.text else \"Missing Subject ID\"\n","\n","                    for writing in root.findall('.//WRITING'):\n","                        text_elem = writing.find('TEXT')\n","\n","                        if text_elem is not None and text_elem.text:\n","                            text = text_elem.text.strip()\n","\n","                            if any(keyword in text.lower() for keyword in keywords):\n","                                if subject_id not in subject_dict:\n","                                    subject_dict[subject_id] = []\n","                                subject_dict[subject_id].append(text)\n","                                classified_count += 1\n","\n","            except ET.ParseError as e:\n","                print(f\"Error parsig {filename}: {e}\")\n","            except Exception as e:\n","                print(f\"An error occurred with {filename}: {e}\")\n","\n","    for subject_id, texts in subject_dict.items():\n","        term_freq = Counter()\n","        combined_texts = ' | '.join(texts)\n","        combined_texts = preprocess_text(combined_texts)\n","        term_freq.update(combined_texts.split())\n","        text_by_freq = term_freq.most_common()\n","        term_freq_str = \", \".join([f\"{term}:{freq}\" for term, freq in text_by_freq])\n","        data.append({'ID': subject_id, 'TEXT': combined_texts, 'Term-Frequency': term_freq_str})\n","\n","    df = pd.DataFrame(data)\n","    return df\n","\n","def plot_sentiment_distribution(df):\n","    sns.countplot(x='Predicted Sentiment', data=df)\n","    plt.title(\"Sentiment Distribution\")\n","    plt.xlabel(\"Sentiment\")\n","    plt.ylabel(\"Count\")\n","    plt.show()\n","def plot_wordcloud(df, sentiment_value, sentiment_label):\n","    sentiment_texts = ' '.join(df[df['Predicted Sentiment'] == sentiment_value]['TEXT'].dropna())\n","\n","    if not sentiment_texts.strip():\n","        print(f\"No text entries found for {sentiment_label} sentiment.\")\n","        return\n","\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sentiment_texts)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.title(f\"Word Cloud for {sentiment_label.capitalize()} Sentiments\")\n","    plt.show()\n","\n","def load_ground_truth(ground_truth_file):\n","    ground_truth = {}\n","    with open(ground_truth_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            if len(parts) == 2:\n","                subject_id, label = parts\n","                ground_truth[subject_id] = int(label)\n","    return ground_truth\n","\n","\n","def evaluate_model(y_true, y_pred):\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_true, y_pred))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_true, y_pred))\n","\n","def derive_pos_tags(df):\n","    pos_data = []\n","    for index, row in df.iterrows():\n","        words = row['TEXT'].split()\n","        pos_tags = nltk.pos_tag(words)\n","        pos_data.extend([(word, tag) for word, tag in pos_tags])\n","    pos_df = pd.DataFrame(pos_data, columns=['Word', 'POS Tag'])\n","    pos_df.to_csv('pathtofile', index=False)\n","    print(\"POS tags saved to pos_tags_weight.csv\")\n","\n","def save_predictions_to_csv(test_df, predictions, output_file):\n","    test_df['Predicted Sentiment'] = predictions\n","    test_df[['ID', 'TEXT', 'Predicted Sentiment', 'Term-Frequency']].to_csv(output_file, index=False)\n","    print(f\"Predictions saved to {output_file}\")\n","\n","folder_path = 'pathtofile'\n","ground_truth_file = 'pathtofile'\n","\n","all_xml_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.xml')]\n","\n","#skipped_files = set(all_xml_files) - set(processed_files)\n","\n","df = extract_text_by_subject_id(folder_path)\n","\n","ground_truth = load_ground_truth(ground_truth_file)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_texts = train_df['TEXT'].tolist()\n","train_labels = [ground_truth[id_] for id_ in train_df['ID']]\n","test_texts = test_df['TEXT'].tolist()\n","test_labels = [ground_truth[id_] for id_ in test_df['ID']]\n","\n","def save_all_sentiments_to_csv(df, model, vectorizer, output_file):\n","    text_vectors = vectorizer.transform(df['TEXT'])\n","    predictions = model.predict(text_vectors)\n","    results_df = df.copy()\n","    results_df['Predicted Sentiment'] = predictions\n","    results_df[['ID', 'TEXT', 'Predicted Sentiment']].to_csv(output_file, index=False)\n","    print(f\"Sentiment predictions saved to {output_file}\")\n","\n","# Classifier setup\n","vectorizer = TfidfVectorizer(max_features=1000)\n","svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n","\n","train_vectors = vectorizer.fit_transform(train_texts)\n","test_vectors = vectorizer.transform(test_texts)\n","\n","svm_model.fit(train_vectors, train_labels)\n","\n","test_predictions = svm_model.predict(test_vectors)\n","\n","print(\"\\nEvaluation on Testing Set:\")\n","evaluate_model(test_labels, test_predictions)\n","\n","# Save predictions\n","output_file = 'pathtofile'\n","save_predictions_to_csv(test_df, test_predictions, output_file)\n","\n","df['Predicted Sentiment'] = svm_model.predict(vectorizer.transform(df['TEXT']))\n","\n","plot_wordcloud(test_df, sentiment_value=1, sentiment_label='positive')\n","plot_wordcloud(test_df, sentiment_value=0, sentiment_label='negative')\n","plot_sentiment_distribution(df)\n","\n","print(f\"Classified count: {classified_count}\")\n"],"metadata":{"id":"Uz1MGkK2E1HP"},"execution_count":null,"outputs":[]}]}